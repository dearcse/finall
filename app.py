import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import joblib
from PIL import Image
import av
import time
from collections import deque
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode, RTCConfiguration

# --- Page Configuration ---
st.set_page_config(page_title="AI Posture Correction Pro", page_icon="ğŸ¢", layout="wide")

# --- CSS & Audio Script ---
def get_audio_html():
    # ë¸Œë¼ìš°ì € ê¸°ë³¸ ì˜¤ë””ì˜¤ ì‚¬ìš© (ê°„ë‹¨í•œ beep)
    js_code = """
        <script>
        function playAlert() {
            var audio = new Audio('https://actions.google.com/sounds/v1/alarms/beep_short.ogg');
            audio.volume = 0.5;
            audio.play();
        }
        </script>
        <div id="audio-container"></div>
    """
    return js_code

st.markdown("""
    <style>
    .big-font { font-size:24px !important; font-weight: bold; }
    .good-text { color: #2ecc71; font-weight: bold; font-size: 30px; }
    .mild-text { color: #f1c40f; font-weight: bold; font-size: 30px; }
    .severe-text { color: #e74c3c; font-weight: bold; font-size: 30px; animation: blink 1s infinite; }
    
    .advice-box {
        background-color: #fff9c4;
        padding: 15px;
        border-radius: 10px;
        border-left: 5px solid #fbc02d;
        font-size: 20px;
        font-weight: bold;
        color: #333;
        margin-top: 10px;
    }

    @keyframes blink {
        50% { opacity: 0.5; }
    }
    
    .stProgress > div > div > div > div {
        background-color: #2ecc71;
    }
    </style>
    """, unsafe_allow_html=True)

st.markdown(get_audio_html(), unsafe_allow_html=True)

st.title("ğŸ¢ AI Posture Correction Pro")
st.markdown("Turn on the webcam to analyze your posture. **First, set your own best posture as the standard.**")

# --- Load Model & MediaPipe (ëª¨ë¸ì€ ë¡œë“œë§Œ í•˜ê³  ì‚¬ìš©ì€ ì•ˆ í•¨: í˜¸í™˜ìš©) ---
@st.cache_resource
def load_model():
    try:
        return joblib.load('posture_model.pkl')
    except:
        return None

model = load_model()
mp_pose = mp.solutions.pose

# --- ê±°ë¦¬ ê¸°ë°˜ í™•ë¥  ê³„ì‚° í•¨ìˆ˜ (Calibration ì „ìš©) ---
def distance_to_probs(distance, t_good=0.12, t_mild=0.28):
    """
    baselineê³¼ì˜ ê±°ë¦¬(distance)ë¥¼ ë°›ì•„
    good / mild / severeì˜ í™•ë¥  ë¶„í¬ë¥¼ ë§Œë“¤ì–´ì„œ ë°˜í™˜.
    t_good: ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ê±°ì˜ good
    t_mild: ì´ ê°’ë³´ë‹¤ í¬ë©´ severeë¡œ ê¸°ìš¸ê¸° ì‹œì‘
    """
    d = float(distance)

    # Good ì ìˆ˜: 0ì—ì„œ t_goodê¹Œì§€ ì„ í˜•ìœ¼ë¡œ ê°ì†Œ
    good_score = max(0.0, 1.0 - d / max(t_good, 1e-6))

    # Mild ì ìˆ˜: t_good ê·¼ì²˜ì—ì„œ ë†’ê³ , 0ê³¼ t_mildì—ì„œ 0ì´ ë˜ë„ë¡
    if d <= t_good:
        mild_score = d / max(t_good, 1e-6)
    elif d <= t_mild:
        mild_score = 1.0 - (d - t_good) / max(t_mild - t_good, 1e-6)
    else:
        mild_score = 0.0

    # Severe ì ìˆ˜: t_mild ì´í›„ë¶€í„° ì¦ê°€
    if d <= t_mild:
        severe_score = 0.0
    else:
        severe_score = min(1.0, (d - t_mild) / max(t_mild, 1e-6))

    scores = {
        "good": good_score,
        "mild": mild_score,
        "severe": severe_score,
    }
    total = sum(scores.values())
    if total <= 0:
        return {"good": 1/3, "mild": 1/3, "severe": 1/3}

    for k in scores:
        scores[k] /= total

    return scores


# --- í¬ì¦ˆ ëœë“œë§ˆí¬ì—ì„œ feature ì¶”ì¶œ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼ ë…¼ë¦¬) ---
def extract_features_from_landmarks(landmarks, img_shape):
    """
    MediaPipe pose_landmarksì™€ ì´ë¯¸ì§€ í¬ê¸°ì—ì„œ
    ì–´ê¹¨ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”ëœ ìƒë°˜ì‹  íŠ¹ì§• ë²¡í„°ì™€ í™”ë©´ì— ì°ì„ í¬ì¸íŠ¸ ì¢Œí‘œë¥¼ ë°˜í™˜.
    """
    # ì™¼/ì˜¤ë¥¸ ì–´ê¹¨
    l_sh = landmarks[11]
    r_sh = landmarks[12]

    center_x = (l_sh.x + r_sh.x) / 2.0
    center_y = (l_sh.y + r_sh.y) / 2.0
    width = np.linalg.norm(
        np.array([l_sh.x, l_sh.y]) - np.array([r_sh.x, r_sh.y])
    )
    if width == 0:
        width = 1.0

    indices = [0, 2, 5, 7, 8, 11, 12]  # ì½”, ëˆˆ, ê·€, ì–´ê¹¨
    features = []

    h, w, _ = img_shape
    keypoints = {}

    for idx in indices:
        lm = landmarks[idx]
        norm_x = (lm.x - center_x) / width
        norm_y = (lm.y - center_y) / width
        features.extend([norm_x, norm_y])
        px, py = int(lm.x * w), int(lm.y * h)
        keypoints[idx] = (px, py)

    return features, keypoints


# --- Real-time Video Processing Class (Calibration + Distance ê¸°ë°˜ íŒë‹¨) ---
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.pose = mp_pose.Pose(
            static_image_mode=False,
            min_detection_confidence=0.5,
            model_complexity=1
        )

        # 1. Baseline (ë‚´ ê¸°ì¤€ ìì„¸)
        self.baseline = None
        self.calibrate_now = False   # ë²„íŠ¼ ëˆŒë ¸ì„ ë•Œ Trueë¡œ ë°”ë€Œê³ , ë‹¤ìŒ í”„ë ˆì„ì—ì„œ baseline ì €ì¥

        # 2. ê±°ë¦¬ smoothing
        self.distance_history = deque(maxlen=10)

        # 3. ê²°ê³¼ ê³µìœ ìš© ë³€ìˆ˜
        self.latest_probs = {'good': 0.0, 'mild': 0.0, 'severe': 0.0}
        self.latest_pred = "good"
        self.latest_distance = 0.0

        # 4. ì‚¬ìš´ë“œìš©
        self.severe_consecutive_frames = 0
        self.trigger_sound = False

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.pose.process(img_rgb)

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark

            try:
                # 1) Feature ì¶”ì¶œ
                features, keypoints = extract_features_from_landmarks(
                    landmarks, img.shape
                )

                # 2) ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë²„íŠ¼ì´ ëˆŒë¦° ê²½ìš° â†’ í˜„ì¬ ìì„¸ë¥¼ baselineìœ¼ë¡œ ì €ì¥
                if self.calibrate_now:
                    self.baseline = np.array(features)
                    self.distance_history.clear()
                    self.calibrate_now = False

                # 3) baselineì´ ì„¤ì •ëœ ê²½ìš° â†’ ê±°ë¦¬ ê³„ì‚° + í™•ë¥ /ë ˆì´ë¸” ì—…ë°ì´íŠ¸
                if self.baseline is not None:
                    diff = np.array(features) - np.array(self.baseline)
                    dist = float(np.linalg.norm(diff))
                    self.distance_history.append(dist)
                    avg_dist = float(np.mean(self.distance_history))

                    self.latest_distance = avg_dist

                    # ê±°ë¦¬ â†’ í™•ë¥  ë¶„í¬
                    prob_dict = distance_to_probs(avg_dist)
                    self.latest_probs = prob_dict
                    self.latest_pred = max(prob_dict, key=prob_dict.get)
                else:
                    # baselineì´ ì•„ì§ ì—†ìœ¼ë©´, ì„ì‹œë¡œ ëª¨ë‘ goodìœ¼ë¡œ
                    self.latest_probs = {'good': 1.0, 'mild': 0.0, 'severe': 0.0}
                    self.latest_pred = 'good'
                    self.latest_distance = 0.0

                current_pred = self.latest_pred

                # 4) Skeleton ì‹œê°í™” (ìƒ‰ìƒ: good=ì´ˆë¡, mild=ë…¸ë‘, severe=ë¹¨ê°•)
                color = (0, 255, 0)  # Green
                if current_pred == 'mild':
                    color = (0, 255, 255)  # Yellow
                if current_pred == 'severe':
                    color = (0, 0, 255)  # Red

                # ì  ì°ê¸°
                for idx, (px, py) in keypoints.items():
                    cv2.circle(img, (px, py), 5, color, -1)

                # ì–´ê¹¨ì„ , ëª©ì„ 
                if 11 in keypoints and 12 in keypoints:
                    cv2.line(img, keypoints[11], keypoints[12], color, 2)
                if 0 in keypoints and 11 in keypoints and 12 in keypoints:
                    sh_center = (
                        (keypoints[11][0] + keypoints[12][0]) // 2,
                        (keypoints[11][1] + keypoints[12][1]) // 2,
                    )
                    cv2.line(img, sh_center, keypoints[0], color, 2)

                # 5) ì‚¬ìš´ë“œ íŠ¸ë¦¬ê±° (severeê°€ ì¼ì • í”„ë ˆì„ ì´ìƒ ì§€ì†ë˜ë©´)
                if current_pred == 'severe':
                    self.severe_consecutive_frames += 1
                    if self.severe_consecutive_frames > 30:  # ëŒ€ëµ 1ì´ˆ ì´ìƒ
                        self.trigger_sound = True
                else:
                    self.severe_consecutive_frames = 0
                    self.trigger_sound = False

            except Exception:
                pass

        return av.VideoFrame.from_ndarray(img, format="bgr24")


# --- UI Layout ---
col_main, col_sidebar = st.columns([3, 1])

ctx = None

with col_main:
    # Calibration Button
    st.markdown("### ğŸ“ Calibration")
    st.markdown("1. í¸ì•ˆí•˜ì§€ë§Œ **ê°€ì¥ ë°”ë¥¸ ìì„¸**ë¥¼ ë§Œë“  ë’¤<br>2. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ í˜„ì¬ ìì„¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”.", unsafe_allow_html=True)

    calib_msg_ph = st.empty()

    # webrtc_streamer ë¨¼ì € ìƒì„±
    if model is None:
        # ëª¨ë¸ì€ ì•ˆ ì“°ì§€ë§Œ, íŒŒì¼ì´ ì—†ì–´ë„ ë¬¸ì œì—†ì´ ë™ì‘í•˜ê²Œ ê·¸ëƒ¥ ì •ë³´ë§Œ
        st.info("Model file (posture_model.pkl) is missing, but calibration-based mode works without it.")
    ctx = webrtc_streamer(
        key="posture-pro",
        video_processor_factory=VideoProcessor,
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True
    )

    # ë²„íŠ¼: í˜„ì¬ í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ baselineì„ ì„¸íŒ…í•˜ë„ë¡ í”Œë˜ê·¸ë§Œ ì¼¬
    if st.button("ğŸ“ Set Current Posture as 'Standard'"):
        if ctx and ctx.video_processor:
            ctx.video_processor.calibrate_now = True
            calib_msg_ph.success("âœ… Standard posture captured! (Hold similar pose when you want 'GOOD')")
        else:
            calib_msg_ph.warning("Webcam is not ready yet. Please wait a moment and try again.")


with col_sidebar:
    st.markdown("### ğŸ“Š Live Status")
    status_ph = st.empty()
    advice_ph = st.empty()
    
    st.write("---")
    st.markdown("### Posture Score (Good %)")
    score_ph = st.empty()
    
    st.write("---")
    dist_ph = st.empty()

    # Hidden placeholder for sound
    sound_ph = st.empty()

# --- Main Loop ---
if ctx and ctx.state.playing:
    while True:
        if not ctx.state.playing:
            break

        if ctx.video_processor:
            probs = ctx.video_processor.latest_probs
            pred = ctx.video_processor.latest_pred
            trigger_sound = ctx.video_processor.trigger_sound
            dist = ctx.video_processor.latest_distance

            # 1. Update Status Text & Advice
            if pred == 'good':
                status_ph.markdown("<div class='good-text'>GOOD ğŸ˜Š</div>", unsafe_allow_html=True)
                advice_ph.markdown("<div class='advice-box'>âœ… Perfect alignment! Keep it up.</div>", unsafe_allow_html=True)
            
            elif pred == 'mild':
                status_ph.markdown("<div class='mild-text'>MILD ğŸ˜</div>", unsafe_allow_html=True)
                advice_ph.markdown("<div class='advice-box'>ğŸ’¡ Lift your head slightly.<br>Relax your shoulders.</div>", unsafe_allow_html=True)
            
            else:  # severe
                status_ph.markdown("<div class='severe-text'>SEVERE ğŸ¢</div>", unsafe_allow_html=True)
                advice_ph.markdown("<div class='advice-box'>ğŸš¨ <b>Pull chin back!</b><br>Open your chest.</div>", unsafe_allow_html=True)
            
            # 2. Update Single Posture Score Bar (Probability of Good)
            good_score = int(probs.get('good', 0) * 100)
            score_ph.progress(good_score, text=f"{good_score}%")

            # 3. Baselineê³¼ì˜ ê±°ë¦¬ í‘œì‹œ (ì°¸ê³ ìš©)
            dist_ph.markdown(f"Current deviation from standard posture: <b>{dist:.3f}</b>", unsafe_allow_html=True)

            # 4. Sound Alert
            if trigger_sound:
                sound_ph.markdown(
                    """
                    <script>
                    playAlert();
                    </script>
                    """,
                    unsafe_allow_html=True,
                )
            else:
                sound_ph.empty()

        time.sleep(0.1)
