import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
from PIL import Image
import av
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode
from collections import deque
import time

# --- Page Configuration ---
st.set_page_config(page_title="AI Real-time Posture Calibration", page_icon="ğŸ¢")

# --- ìŠ¤íƒ€ì¼ ì„¤ì • ---
st.markdown(
    """
    <style>
    .big-font { font-size:24px !important; font-weight: bold; }
    .good-text { color: #2ecc71; font-weight: bold; font-size: 20px;}
    .mild-text { color: #f1c40f; font-weight: bold; font-size: 20px;}
    .severe-text { color: #e74c3c; font-weight: bold; font-size: 20px;}
    .warning-box { background-color: #fadbd8; border: 2px solid #e74c3c; padding: 15px; border-radius: 10px; text-align: center; }
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ¢ AI Real-time Turtle Neck Calibration")
st.write("First, hold your **best posture** for a few seconds. The app will use it as your personal standard.")

mp_pose = mp.solutions.pose


# --- ê³µí†µ Feature ì¶”ì¶œ í•¨ìˆ˜ (í•™ìŠµ ë•Œì™€ ë™ì¼í•œ ë°©ì‹) ---
def extract_features_from_landmarks(landmarks, img_shape):
    """
    MediaPipe pose_landmarksì™€ ì´ë¯¸ì§€ í¬ê¸°ì—ì„œ
    ì–´ê¹¨ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”ëœ ìƒë°˜ì‹  íŠ¹ì§• ë²¡í„°ì™€ í™”ë©´ì— ì°ì„ í¬ì¸íŠ¸ ì¢Œí‘œë¥¼ ë°˜í™˜.
    """
    # ì™¼/ì˜¤ë¥¸ ì–´ê¹¨
    l_sh = landmarks[11]
    r_sh = landmarks[12]

    center_x = (l_sh.x + r_sh.x) / 2.0
    center_y = (l_sh.y + r_sh.y) / 2.0
    width = np.linalg.norm(
        np.array([l_sh.x, l_sh.y]) - np.array([r_sh.x, r_sh.y])
    )
    if width == 0:
        width = 1.0

    indices = [0, 2, 5, 7, 8, 11, 12]  # ì½”, ëˆˆ, ê·€, ì–´ê¹¨
    features = []

    h, w, _ = img_shape
    draw_points = []

    for idx in indices:
        lm = landmarks[idx]
        norm_x = (lm.x - center_x) / width
        norm_y = (lm.y - center_y) / width
        features.extend([norm_x, norm_y])
        draw_points.append((int(lm.x * w), int(lm.y * h)))

    return features, draw_points


# --- ê±°ë¦¬ ê¸°ë°˜ í™•ë¥  ê³„ì‚° í•¨ìˆ˜ (fuzzy membership ë¹„ìŠ·í•˜ê²Œ) ---
def distance_to_probs(distance, t_good=0.12, t_mild=0.28):
    """
    baselineê³¼ì˜ ê±°ë¦¬(distance)ë¥¼ ë°›ì•„
    good / mild / severeì˜ í™•ë¥  ë¶„í¬ë¥¼ ë§Œë“¤ì–´ì„œ ë°˜í™˜.
    t_good: ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ê±°ì˜ good
    t_mild: ì´ ê°’ë³´ë‹¤ í¬ë©´ severeë¡œ ê¸°ìš¸ê¸° ì‹œì‘
    """
    d = float(distance)

    # Good ì ìˆ˜: 0ì—ì„œ t_goodê¹Œì§€ ì„ í˜•ìœ¼ë¡œ ê°ì†Œ
    good_score = max(0.0, 1.0 - d / max(t_good, 1e-6))

    # Mild ì ìˆ˜: t_good ê·¼ì²˜ì—ì„œ ë†’ê³ , 0ê³¼ t_mildì—ì„œ 0ì´ ë˜ë„ë¡
    if d <= t_good:
        mild_score = d / max(t_good, 1e-6)
    elif d <= t_mild:
        mild_score = 1.0 - (d - t_good) / max(t_mild - t_good, 1e-6)
    else:
        mild_score = 0.0

    # Severe ì ìˆ˜: t_mild ì´í›„ë¶€í„° ì¦ê°€
    if d <= t_mild:
        severe_score = 0.0
    else:
        # t_mild ì´í›„ë¡œ ì ì  1ì— ê°€ê¹Œì›Œì§€ë„ë¡
        severe_score = min(1.0, (d - t_mild) / max(t_mild, 1e-6))

    scores = {
        "good": good_score,
        "mild": mild_score,
        "severe": severe_score,
    }
    total = sum(scores.values())
    if total <= 0:
        # ì „ë¶€ 0ì´ë©´ ê· ë“±ë¶„í¬
        return {"good": 1 / 3, "mild": 1 / 3, "severe": 1 / 3}

    # ì •ê·œí™”
    for k in scores:
        scores[k] /= total

    return scores


# --- Real-time Video Processing Class ---
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.pose = mp_pose.Pose(
            static_image_mode=False,
            min_detection_confidence=0.5,
            model_complexity=1,
        )

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê´€ë ¨
        self.collecting_baseline = True
        self.baseline_buffer = []
        self.baseline = None

        # ê±°ë¦¬ smoothing
        self.distance_history = deque(maxlen=10)

        # ì‹¤ì‹œê°„ ìƒíƒœ ê³µìœ ìš©
        self.latest_probs = {"good": 0.0, "mild": 0.0, "severe": 0.0}
        self.latest_pred = None
        self.latest_distance = 0.0

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")

        # 1. MediaPipe ì²˜ë¦¬
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.pose.process(img_rgb)

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark

            try:
                # 2. Feature ì¶”ì¶œ
                features, draw_points = extract_features_from_landmarks(
                    landmarks, img.shape
                )

                # 3. Calibration / Distance ê³„ì‚°
                if self.collecting_baseline:
                    # baseline ìˆ˜ì§‘ ë‹¨ê³„
                    self.baseline_buffer.append(features)
                    # 20í”„ë ˆì„ ì •ë„ ëª¨ìœ¼ë©´ í‰ê· ì„ baselineìœ¼ë¡œ ì‚¬ìš©
                    if len(self.baseline_buffer) >= 20:
                        self.baseline = np.mean(self.baseline_buffer, axis=0)
                        self.collecting_baseline = False
                        self.distance_history.clear()
                elif self.baseline is not None:
                    # baselineì´ ì¤€ë¹„ëœ ì´í›„ â†’ í˜„ì¬ ìì„¸ì™€ ê±°ë¦¬ ê³„ì‚°
                    diff = np.array(features) - np.array(self.baseline)
                    dist = float(np.linalg.norm(diff))
                    self.distance_history.append(dist)
                    avg_dist = float(np.mean(self.distance_history))

                    self.latest_distance = avg_dist

                    # ê±°ë¦¬ â†’ good/mild/severe í™•ë¥  ë¶„í¬
                    prob_dict = distance_to_probs(avg_dist)
                    self.latest_probs = prob_dict

                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ predë¡œ ì‚¬ìš©
                    self.latest_pred = max(prob_dict, key=prob_dict.get)

                # 4. í™”ë©´ì—ëŠ” ì ë§Œ ì°ê¸°
                for px, py in draw_points:
                    cv2.circle(img, (px, py), 5, (0, 255, 0), -1)

            except Exception:
                # ì—ëŸ¬ ë°œìƒ ì‹œ í”„ë ˆì„ë§Œ ê·¸ëŒ€ë¡œ ë°˜í™˜
                pass

        return av.VideoFrame.from_ndarray(img, format="bgr24")


# --- Main Tab Configuration ---
tab1, tab2 = st.tabs(["ğŸ“· Real-time (Calibrated)", "ğŸ–¼ï¸ Upload Photo (disabled)"])

# Tab 1: Real-time with Calibration
with tab1:
    st.header("Real-time Webcam (Personal Calibration)")

    col1, col2 = st.columns([2, 1])

    # ì™¼ìª½: ì›¹ìº 
    with col1:
        ctx = webrtc_streamer(
            key="posture-calibration",
            video_processor_factory=VideoProcessor,
            mode=WebRtcMode.SENDRECV,
            media_stream_constraints={"video": True, "audio": False},
            rtc_configuration={
                "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
            },
            async_processing=True,
        )

    # ì˜¤ë¥¸ìª½: ìƒíƒœ í‘œì‹œ
    with col2:
        st.subheader("Live Status")

        calib_text_ph = st.empty()
        status_text_ph = st.empty()

        st.write("**Prediction Confidence:**")

        # ë¼ë²¨ (Good / Mild / Severe)
        label_good, label_mild, label_severe = st.columns(3)

        with label_good:
            st.markdown(
                "<p style='text-align: center; color: #2ecc71; font-weight: bold;'>Good</p>",
                unsafe_allow_html=True,
            )

        with label_mild:
            st.markdown(
                "<p style='text-align: center; color: #f1c40f; font-weight: bold;'>Mild</p>",
                unsafe_allow_html=True,
            )

        with label_severe:
            st.markdown(
                "<p style='text-align: center; color: #e74c3c; font-weight: bold;'>Severe</p>",
                unsafe_allow_html=True,
            )

        # ê°€ë¡œ Progress bar (ì „ì²´ í­)
        st.write("Good:")
        bar_good_ph = st.empty()

        st.write("Mild:")
        bar_mild_ph = st.empty()

        st.write("Severe:")
        bar_severe_ph = st.empty()

        warning_ph = st.empty()
        distance_ph = st.empty()

    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë£¨í”„
    if ctx and ctx.state.playing:
        while True:
            if not ctx.state.playing:
                break

            vp = ctx.video_processor

            if vp is not None:
                # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒíƒœ í‘œì‹œ
                if vp.collecting_baseline or vp.baseline is None:
                    calib_text_ph.info(
                        "ğŸ§­ Calibratingâ€¦ Please hold your **best neutral posture**."
                    )
                else:
                    calib_text_ph.success(
                        "âœ… Calibration complete! Now analyzing your posture."
                    )

                probs = vp.latest_probs
                pred = vp.latest_pred
                dist = vp.latest_distance

                # distance í‘œì‹œ (ì°¸ê³ ìš©)
                if vp.baseline is not None:
                    distance_ph.markdown(
                        f"<p>Current deviation from baseline: <b>{dist:.3f}</b></p>",
                        unsafe_allow_html=True,
                    )
                else:
                    distance_ph.empty()

                if pred is not None:
                    p_good = int(probs.get("good", 0.0) * 100)
                    p_mild = int(probs.get("mild", 0.0) * 100)
                    p_severe = int(probs.get("severe", 0.0) * 100)

                    # ìƒíƒœ í…ìŠ¤íŠ¸
                    if pred == "good":
                        status_text_ph.markdown(
                            "<p class='good-text'>Status: GOOD ğŸ˜Š</p>",
                            unsafe_allow_html=True,
                        )
                    elif pred == "mild":
                        status_text_ph.markdown(
                            "<p class='mild-text'>Status: MILD ğŸ˜</p>",
                            unsafe_allow_html=True,
                        )
                    else:
                        status_text_ph.markdown(
                            "<p class='severe-text'>Status: SEVERE ğŸ¢</p>",
                            unsafe_allow_html=True,
                        )

                    # Progress bars
                    bar_good_ph.progress(p_good, text=f"Good: {p_good}%")
                    bar_mild_ph.progress(p_mild, text=f"Mild: {p_mild}%")
                    bar_severe_ph.progress(p_severe, text=f"Severe: {p_severe}%")

                    # Warning box
                    if pred == "severe":
                        warning_ph.markdown(
                            """
                            <div class='warning-box'>
                                ğŸš¨ <b>BAD POSTURE DETECTED!</b><br>
                                Please straighten your neck.
                            </div>
                            """,
                            unsafe_allow_html=True,
                        )
                    else:
                        warning_ph.empty()
                else:
                    # ì•„ì§ baselineë§Œ ëª¨ìœ¼ëŠ” ì¤‘ì´ê±°ë‚˜, ì •ë³´ ë¶€ì¡±
                    status_text_ph.markdown(
                        "<p>Waiting for stable posture...</p>",
                        unsafe_allow_html=True,
                    )
                    bar_good_ph.progress(0, text="Good: 0%")
                    bar_mild_ph.progress(0, text="Mild: 0%")
                    bar_severe_ph.progress(0, text="Severe: 0%")
                    warning_ph.empty()

            time.sleep(0.1)

# Tab 2: Upload (í˜„ì¬ ë¹„í™œì„±í™”)
with tab2:
    st.header("Upload Photo Diagnosis (Disabled in Calibration Mode)")
    st.info(
        "This prototype focuses on **real-time calibrated analysis**.\n\n"
        "Please use the **Real-time (Calibrated)** tab to analyze your posture "
        "relative to your own best baseline."
    )


